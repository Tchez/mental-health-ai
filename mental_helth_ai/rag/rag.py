from rich import print

from mental_helth_ai.rag.database.db_interface import DatabaseInterface
from mental_helth_ai.rag.llm.llm_interface import LLMInterface


class RAGFactory:
    """Factory class for the RAG model.

    Attributes:
        retriever (DatabaseInterface): The retriever instance to use.
        llm (LLMInterface): The language model instance to use.

    Examples:
        >>> from mental_helth_ai.rag.database.faiss_db_impl import FAISSDatabase
        >>> from mental_helth_ai.rag.llm.ollama_impl import OllamaLLM
        >>> retriever = FAISSDatabase()
        >>> llm = OllamaLLM()
        >>> rag_factory = RAGFactory(retriever=retriever, llm=llm)
        >>> query = 'Responda em um parágrafo, o que é o Transtorno de Déficit de Atenção/Hiperatividade (TDAH)?'
        >>> response = rag_factory.generate_response(query)
        >>> print(f'Response: {response}')
    """  # noqa: E501

    def __init__(self, vector_db: DatabaseInterface, llm: LLMInterface):
        self.vector_db = vector_db
        self.llm = llm

    def generate_response(
        self, query: str, top_k: int = 5
    ) -> tuple[str, list]:
        """Generates a response to a given query using the RAG model.

        Args:
            query (str): The query to generate a response for.
            top_k (int, optional): The number of documents to retrieve from the database. Defaults to 5.

        Raises:
            e: Any exception that occurs during the process.

        Returns:
            tuple[str, list]: The response generated by the RAG model and the list of retrieved documents.
        """  # noqa: E501
        try:
            is_ok = self.vector_db.verify_database()

            if not is_ok:
                print('[red]O banco de dados não está disponível![/red]')
                raise Exception('Database not available or empty.')

            retrieved_documents = self.vector_db.search(query, limit=top_k)

            context = '\n'.join([
                str(doc.properties) for doc in retrieved_documents
            ])
            # TODO: Criar handler para tratar contexto com base no tipo do doc

            system_context = f"Papel: Você é um chatbot especializado em saúde mental que receberá um 'Contexto' com informações verídicas relacionadas à pergunta do usuário, que são provenientes de uma base de dados de fontes confiáveis. Você não é um profissional de saúde e não pode fornecer diagnósticos ou tratamentos, mas utiliza o Contexto para fornecer informações embasadas. Sempre que for responder, diga de qual artigo, livro ou documento veio a informação. \n\nContexto:{context}"  # noqa: E501
            messages = [
                ('system', system_context),
                ('human', f'Pergunta: {query}{context}\n\nAnswer:'),
            ]

            response = self.llm.generate_response(messages)
            return response, retrieved_documents
        except Exception as e:
            raise e


if __name__ == '__main__':
    from mental_helth_ai.rag.database.weaviate_impl import WeaviateClient
    from mental_helth_ai.rag.llm.openai_impl import OpenAILLM

    # from mental_helth_ai.rag.llm.ollama_impl import OllamaLLM

    vector_db = WeaviateClient()
    llm = OpenAILLM()
    # llm = OllamaLLM()

    rag_factory = RAGFactory(vector_db=vector_db, llm=llm)

    query = 'Responda em um parágrafo, o que é o Transtorno de Déficit de Atenção/Hiperatividade (TDAH)?'  # noqa: E501

    try:
        # Pergunta utilizando RAG
        response_rag, _ = rag_factory.generate_response(query)
        print(f'Resposta RAG:\n{response_rag}')

        # Pergunta utilizando apenas o LLM
        response_llm = llm.generate_response(query)
        print(f'Resposta LLM:\n{response_llm}')
    except Exception as e:
        print(f'Error: {e}')
